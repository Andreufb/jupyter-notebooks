{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9468a857",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-start;\">\n",
    "    <div style=\"flex: 0 0 auto; margin-bottom: 0; margin-top: 10px;\">\n",
    "        <img src=\"SAGE_logo.jpeg\" alt=\"NSF SAGE Logo\" width=\"150\"/>\n",
    "    </div>\n",
    "    <div style=\"flex: 0 0 auto; margin-left: auto; margin-bottom: 0;\">\n",
    "        <img src=\"https://www.sci.utah.edu/images/news/2023/sci-30-multi.jpg\" alt=\"Scientific Computing and Imaging Institute Logo\" width=\"100\"/>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: center; margin-top: 0;\">NSF National Data Platform (NDP)</h1>\n",
    "\n",
    "<h3 style=\"text-align: center; margin-top: 0;\">Streaming Data from SAGE Pilot</h3>\n",
    "\n",
    "This use case serves as a practical demonstration of NDP's streaming capabilities, specifically in its integration with the NSF SAGE platform. It primarily focuses on real-time data collection from sensors located at SAGE [node W023](https://portal.sagecontinuum.org/node/W023). This data is then streamed through a broker system. Subsequently, the system compiles this streamed data into historical data files. These files are not only preserved in an archive but are also made easily discoverable and accessible through the comprehensive services offered by NDP. This setup exemplifies the seamless fusion of real-time data acquisition and long-term data storage, enhancing both the immediacy and the utility of the information gathered from the sensors.\n",
    "\n",
    "![SAGE NDP diagram](schema.png)\n",
    "\n",
    "The figure above outlines a data processing workflow of the use case, utilizing an architecture that connects several components. \n",
    "\n",
    "1. **SAGE**: Represents a software-defined sensor network designed for AI at the edge. Data is retried through its API.\n",
    "2. **API-baed Stream Adapter**: This component acts as an adapter that connects the SAGE platform with NDP. Its function is to receive data from SAGE and convert it into an appropriate format.\n",
    "3. **R-Pulsar Framework**: It handles the data in real time, making decisions based on the data received from the API-based Data Adapter.\n",
    "4. **Data Sink Adapter**: It converts the data streams from the R-Pulsar framework into files for archival and registration into the NDP metadata catalog (CKAN). \n",
    "5. **Metadata Catalog**: As data flows through the system, relevant metadata is logged in a catalog. This step is crucial for data management, as it provides information on the data's origin, its structure, and how and when it was processed.\n",
    "6. **User & OSDF Origin**: Users can subscribe to specific data streams within the data streaming platform,or access the data from arhived files (containing the sensor values during a given time window). A POSIX filesystem and/or OSDF Origin is the data access system for historical data.\n",
    "\n",
    "<center>\n",
    "    <div style=\"text-align: right; padding: 5px;\">\n",
    "        <p style=\"text-align: right;\"><strong>Contact:</strong> Scientific and Computing Imaging Institute, University of Utah (<a href=\"mailto:ivan.rodero@utah.edu\">ivan.rodero@utah.edu</a>)</p>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"https://new.nsf.gov/themes/custom/nsf_theme/components/images/logo/logo-desktop.svg\" alt=\"NSF Logo\" width=\"120\" style=\"margin-right: 10px; vertical-align: middle;\">\n",
    "    <span style=\"font-size: 10px; margin-top:10px;\">The National Data Platform was funded by NSF 2333609 under CI, CISE Research Resources programs. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funders.</span>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62320320-4ad6-4f4d-8cee-1d2e9873d39e",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059006d-413f-4340-a853-bbdb53258335",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python plotly nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f81c0d",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "* `import json`: This imports the JSON library, which is used for parsing JSON data.\n",
    "* `from kafka import KafkaConsumer`: Imports the KafkaConsumer class from the Kafka library, which allows us to consume messages from a Kafka topic.\n",
    "* `import plotly.graph_objects as go`: Imports the graph objects module from Plotly, a library for creating interactive visualizations.\n",
    "* `from IPython.display import display, clear_output`: These functions from IPython are used for displaying output in Jupyter Notebooks and clearing the output respectively.\n",
    "* `from datetime import datetime`: Imports the datetime class for handling date and time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252430d-d011-4028-8aed-f1f0e7335016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saleem/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa6338",
   "metadata": {},
   "source": [
    "### Obtain Kafka Settings from NDP metadata registry (CKAN)\n",
    "\n",
    "* `bootstrap_server`: This is the address of the Kafka server.\n",
    "* `topic`: The Kafka topic to consume data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17fda3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Server: ds2.datacollaboratory.org:9093\n",
      "Topic: sage_w023\n"
     ]
    }
   ],
   "source": [
    "CKAN_URL = 'http://ckan.geosciframe.org:5000/catalog'\n",
    "\n",
    "def extract_bootstrap_server_and_topic(dataset_name):\n",
    "    response = requests.get(f\"{CKAN_URL}/api/3/action/package_show\", params={\"id\": dataset_name})\n",
    "    if response.status_code == 200:\n",
    "        dataset_info = response.json().get('result')\n",
    "        if dataset_info and 'resources' in dataset_info:\n",
    "            for resource in dataset_info['resources']:\n",
    "                if resource.get('format') == 'kafka':\n",
    "                    resource_name = resource.get('name')\n",
    "                    parts = resource_name.split('://')\n",
    "                    if len(parts) == 2:\n",
    "                        protocol, server_topic = parts\n",
    "                        if protocol == 'kafka':\n",
    "                            bootstrap_server, topic = server_topic.split('/')\n",
    "                            return bootstrap_server, topic\n",
    "    return None, None\n",
    "\n",
    "dataset_name = \"sage-w023\"\n",
    "bootstrap_server, topic = extract_bootstrap_server_and_topic(dataset_name)\n",
    "\n",
    "# Print bootstrap server and topic\n",
    "print(\"Bootstrap Server:\", bootstrap_server)\n",
    "print(\"Topic:\", topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13aaa9",
   "metadata": {},
   "source": [
    "### Initializing Kafka Consumer\n",
    "The KafkaConsumer is initialized with several parameters:\n",
    "\n",
    "* `topic`: The topic to subscribe to.\n",
    "* `value_deserializer`: A lambda function to deserialize the data (convert it from JSON format).\n",
    "* `enable_auto_commit`: Set to False to manually control message offset commits.\n",
    "* `auto_offset_reset`: Offset reset behavior, latest offset (default) or earliest (reset the offset to the earliest offset available on the broker).\n",
    "* `bootstrap_servers`: The server to connect to for consuming messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e0d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_offset_value = 'earliest'  # Set this to 'earliest' or 'latest' based on your requirement\n",
    "\n",
    "consumer = KafkaConsumer(topic,\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    enable_auto_commit=False,\n",
    "    auto_offset_reset=auto_offset_value,  # Use the variable here                     \n",
    "    bootstrap_servers=bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25dbb7",
   "metadata": {},
   "source": [
    "### Data Storage Lists\n",
    "\n",
    "* `timestamps`: A list to store the timestamps of the data points.\n",
    "* `temperatures`: A list to store the temperature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb3e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "temperatures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b0aa4",
   "metadata": {},
   "source": [
    "### The update_plot Function\n",
    "This function creates and updates the plot:\n",
    "\n",
    "* A Plotly figure (`fig`) is created.\n",
    "* A scatter plot trace is added to the figure, with timestamps on the x-axis and temperatures on the y-axis.\n",
    "* The layout of the figure is updated with titles, axis labels, and other styling elements.\n",
    "* Finally, `fig.show()` displays the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed78a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot():\n",
    "    # Create a Plotly graph object for plotting\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter plot trace to the figure\n",
    "    fig.add_trace(go.Scatter(x=timestamps, y=temperatures, mode='lines+markers',\n",
    "                             line_shape='spline'))\n",
    "\n",
    "    # Update layout of the figure\n",
    "    fig.update_layout(\n",
    "        title='W023 Temperature Data',\n",
    "        xaxis_title='Timestamp',\n",
    "        yaxis_title='Temperature (°C)',\n",
    "        xaxis=dict(showline=True, showgrid=True, showticklabels=True, \n",
    "                   linecolor='rgb(204, 204, 204)', linewidth=2, ticks='outside',\n",
    "                   tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n",
    "        yaxis=dict(showline=True, showgrid=True, showticklabels=True,\n",
    "                   linecolor='rgb(204, 204, 204)', linewidth=2),\n",
    "        autosize=False, margin=dict(autoexpand=False, l=100, r=20, t=110),\n",
    "        showlegend=False, plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562fdce",
   "metadata": {},
   "source": [
    "### The Main Loop\n",
    "The script enters a loop to continuously consume messages from Kafka:\n",
    "\n",
    "* Each message is loaded as a JSON object.\n",
    "* The timestamp and temperature are extracted and processed. The timestamp is formatted to a more readable form.\n",
    "* A check is performed to ensure data is used only if the sensor is 'bme680'.\n",
    "* Data points (timestamps and temperatures) are appended to their respective lists.\n",
    "* The `update_plot` function is called to update the plot.\n",
    "* `clear_output(wait=True)` clears the output and waits for new data.\n",
    "\n",
    "#### Exception Handling\n",
    "* `except KeyboardInterrupt`: Catches a manual interruption (like Ctrl+C) to gracefully exit the loop.\n",
    "* `finally`: Ensures that the Kafka consumer is closed properly when exiting the loop.\n",
    "\n",
    "#### Usage\n",
    "This script is typically used for real-time data monitoring. For instance, you might use it to monitor temperature data from a sensor network, where data is published to a Kafka topic. The script continuously updates a live plot as new data arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de716d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for message in consumer:\n",
    "        message_value = json.loads(message.value)\n",
    "        timestamp = message_value['timestamp']\n",
    "        temperature = message_value['value']\n",
    "\n",
    "        # Split timestamp to separate date and nanoseconds\n",
    "        datetime_part, nanoseconds_part = timestamp[:-1].split('.')\n",
    "        # Parse the datetime string to a datetime object\n",
    "        datetime_obj = datetime.strptime(datetime_part, '%Y-%m-%dT%H:%M:%S')\n",
    "        # Format the datetime object as a string without milliseconds\n",
    "        formatted_timestamp = datetime_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        metadata = message_value['meta']\n",
    "\n",
    "        # Check if the sensor is 'bme680' and append data\n",
    "        if metadata['sensor'] == 'bme680':\n",
    "            timestamps.append(formatted_timestamp)\n",
    "            temperatures.append(temperature)\n",
    "\n",
    "            # Update and display the plot\n",
    "            update_plot()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Handle manual interruption\n",
    "    pass\n",
    "finally:\n",
    "    # Close the Kafka consumer \n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fbaa2-c2f6-4c88-8121-7682e0e80b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
