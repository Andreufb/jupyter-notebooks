{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4db12-a015-4cd4-81dd-6253f77110aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Created on Mon Jan 22 11:35:46 2024\n",
    "\n",
    "@author: Michaela Alksne\n",
    "\n",
    "Script to train a resnet-18 CNN to classify A and B calls in 30 second spectrograms\n",
    "sets model and spectrogram parameters and connects to wandB so user can monitor training progress\n",
    "\n",
    "Model parameters: \n",
    "    - multi-target model: 3 labels per sample\n",
    "    - classification with ResampleLoss function\n",
    "    - weights pretrained on ImageNet\n",
    "    - learning rate = 0.001\n",
    "    - cooling factor = 0.3 (decreases learning rate by multiplying 0.001*3 every ten epochs)\n",
    "    - epochs = 12 \n",
    "    - batch_size = 12\n",
    "\n",
    "Spectrogram parameters:\n",
    "    - 30 second windows\n",
    "    - 3200 Hz(samples/second) sampling rate \n",
    "    - 3200 point-FFT which results in 1 Hz bins\n",
    "    - 90 % overlap (or 1400 samples), resulting in 0.05 second bins\n",
    "    - 1600 Hamming window samples. A Hamming window is used to smooth the signal and reduce spectral leakage/artifacts for the FFT. \n",
    "    - minimum frequency: 10 Hz\n",
    "    - maximum frequency: 150 Hz\n",
    "    \n",
    "Spectrogram augmentations: \n",
    "    - frequency_mask: adds random horizontal bars over image\n",
    "    - time_mask: adds random vertical bars over the image\n",
    "    - add_noise: adds random Gaussian noise to image \n",
    "    \n",
    "Notes for user:\n",
    "batch_size – number of training files to load/process before re-calculating the loss function and backpropagation\n",
    "num_workers – parallelization (ie, cores or cpus)\n",
    "log_interval – interval in epochs to evaluate model with validation dataset and print metrics to the log\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import opensoundscape\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import librosa\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "# read in train and validation dataframes\n",
    "train_clips = pd.read_csv('../labeled_data/train_val_test_clips/train_clips.csv', index_col=[0,1,2]) \n",
    "val_clips = pd.read_csv('../labeled_data/train_val_test_clips/val_clips.csv', index_col=[0,1,2]) \n",
    "print(train_clips.sum()) \n",
    "print(val_clips.sum())\n",
    "\n",
    "calls_of_interest = [\"A NE Pacific\", \"B NE Pacific\"] #define the calls for CNN\n",
    "model = opensoundscape.CNN('resnet18',classes=calls_of_interest,sample_duration=30.0, single_target=False) # create a CNN object designed to recognize 30-second samples\n",
    "opensoundscape.ml.cnn.use_resample_loss(model, train_df=train_clips) # loss function for mult-target classification\n",
    "\n",
    "# moodify model preprocessing for making spectrograms the way I want them\n",
    "model.preprocessor.pipeline.to_spec.params.window_type = 'hamming'\n",
    "model.preprocessor.pipeline.to_spec.params.window_samples = 1600 \n",
    "model.preprocessor.pipeline.to_spec.params.overlap_samples = 1400 \n",
    "model.preprocessor.pipeline.to_spec.params.fft_size = 3200 \n",
    "model.preprocessor.pipeline.to_spec.params.decibel_limits = (-120,150)\n",
    "model.preprocessor.pipeline.to_spec.params.scaling = 'density'\n",
    "model.preprocessor.pipeline.bandpass.params.min_f = 10\n",
    "model.preprocessor.pipeline.bandpass.params.max_f = 150\n",
    "model.preprocessor.pipeline.frequency_mask.bypass = True\n",
    "#model.preprocessor.pipeline.time_mask.bypass = True\n",
    "#model.preprocessor.pipeline.frequency_mask.set(max_width = 0.003, max_masks=1)\n",
    "model.preprocessor.pipeline.time_mask.set(max_width = 0.1, max_masks=5)\n",
    "model.preprocessor.pipeline.add_noise.set(std=0.1)\n",
    "model.preprocessor.pipeline.random_affine.bypass=True\n",
    "model.optimizer_params['lr']=0.001\n",
    "model.lr_cooling_factor = 0.3 \n",
    "model.wandb_logging['n_preview_samples']=100 # number of samples to look at in wandB\n",
    "\n",
    "\n",
    "# wandb_session = wandb.init( #initialize wandb logging \n",
    "#         entity='BigBlueWhale', #replace with your entity/group name\n",
    "#         project='Sonobuoy Model',\n",
    "#         name='Trial 9: 30 second windows')\n",
    "\n",
    "model.train(\n",
    "    train_clips, \n",
    "    val_clips, \n",
    "    epochs = 12, \n",
    "    batch_size= 128, \n",
    "    log_interval=1, #log progress every 1 batches\n",
    "    num_workers = 12, \n",
    "    #wandb_session=wandb_session,\n",
    "    save_interval = 1, #save checkpoint every 1 epoch\n",
    "    save_path = '../model_states' #location to save checkpoints (epochs)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f01cb-e0c3-4dc6-a916-769acf68825b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
