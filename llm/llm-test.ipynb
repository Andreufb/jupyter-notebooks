{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1da733-7d5b-4c6a-815b-344dcc627a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "os.environ['HF_HOME']='/srv/starter_content/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfa3a4-ed7a-461a-a335-09cfeafc2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"eci-io/climategpt-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d7dbf-fc29-4cc4-a9a0-50ade36ba1b3",
   "metadata": {},
   "source": [
    "## NDP LLM Service Documentation\n",
    "\n",
    "This Python code snippet is designed to launch various components of a chat service named \"FastChat.\" Each function starts a different part of the service using the `subprocess.run` method to execute shell commands.\n",
    "\n",
    "### `run_controller()`\n",
    "\n",
    "Starts the controller for the FastChat service, responsible for managing and coordinating different parts of the service.\n",
    "\n",
    "```python\n",
    "def run_controller():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"127.0.0.1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153eb97-e79f-4592-8a49-5ce4ff1efe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_controller():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"127.0.0.1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730d1d8-8a13-4b7c-907c-03b948f6633b",
   "metadata": {},
   "source": [
    "## `run_worker`\n",
    "Initiates a model worker for processing and generating responses based on specified models. Runs the model worker module, specifying the local host and a list of model names for processing requests. The --model-path argument should point to the directory where the models are stored.\n",
    "```python \n",
    "def run_model_worker():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.model_worker\", \"--host\", \"127.0.0.1\", \"--model-names\", \"gpt-4-turbo,Trendyol-LLM-7b-chat-v0.1,text-embedding-ada-002\", \"--model-path\", model])\n",
    "\n",
    "```\n",
    "### `run_api`\n",
    "\n",
    "Launches an API server that handles API requests to the FastChat service.\n",
    "Runs the API server module on the local host, acting as an interface between the service and external clients or applications.\n",
    "```python\n",
    "def run_api_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"127.0.0.1\"])\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879fbbe-fdfc-42a4-b4ed-cbab96c2a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_worker():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.model_worker\", \"--host\", \"127.0.0.1\", \"--model-names\", \"eci-io/climategpt-7b,text-embedding-ada-002\", \"--model-path\", model])\n",
    "\n",
    "def run_api_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"127.0.0.1\"])\n",
    "def run_ui_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.gradio_web_server\", \"--host\", \"127.0.0.1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a7ae9-3ab4-460c-baf2-80a0ae43a45a",
   "metadata": {},
   "source": [
    "## Starting the `run_controller` Function in a Separate Thread\n",
    "\n",
    "To enable the FastChat controller to run concurrently with the main program, the `run_controller` function is executed in a separate thread. This is achieved using Python's `threading` module, which allows for the execution of code in parallel to the main execution flow of the program.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "controller_thread = threading.Thread(target=run_controller)\n",
    "controller_thread.start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629becb-dfcd-4262-be6c-d3606e711ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_thread = threading.Thread(target=run_controller)\n",
    "controller_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789765a-af59-4fc7-80a7-06b32114b829",
   "metadata": {},
   "source": [
    "## Starting the `run_model_worker` Function in a Separate Thread\n",
    "\n",
    "To facilitate concurrent execution of the FastChat model worker alongside the main program and potentially other service components, the `run_model_worker` function is executed in a separate thread. This concurrent execution is made possible through the use of Python's `threading` module.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "model_worker_thread = threading.Thread(target=run_model_worker)\n",
    "model_worker_thread.start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3ac13-42c2-4420-a4cd-476cf633b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4eab9-b566-4230-8c5f-8fd73d6a8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_worker_thread = threading.Thread(target=run_model_worker)\n",
    "model_worker_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdd025-0d10-4b2a-94c5-a2fe681cf1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec420dc4-bfb8-4a29-b34b-1bab4a203385",
   "metadata": {},
   "source": [
    "## Running the `run_api_server` Function in a Separate Thread\n",
    "\n",
    "To ensure the API server component of the FastChat service operates concurrently with other parts of the application, the `run_api_server` function is launched in a separate thread. This concurrency is achieved with the help of Python's `threading` module, allowing multiple components to run simultaneously, improving scalability and responsiveness.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "api_server_thread = threading.Thread(target=run_api_server)\n",
    "api_server_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d6983-f16e-417e-b648-fc918917c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_server_thread = threading.Thread(target=run_api_server)\n",
    "api_server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889395f9-a7b1-413e-b67b-8f494abd5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl  -X 'GET' \\\n",
    "  'http://localhost:8000/v1/models' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee017faf-39cc-40f4-8347-4c9e7b77b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8000/v1/chat/completions  -H \"Content-Type: application/json\"  -d '{ \"model\": \"eci-io/climategpt-7b\", \"messages\": [{\"role\": \"user\", \"content\": \"Who is Fumio Kishida?\"}]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066f108-e7fd-4d29-b080-e8b3b654aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"eci-io/climategpt-7b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Who is Fumio Kishida?\"}],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c4d98-8b10-40b6-869b-af634dc18e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/api/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"eci-io/climategpt-7b\",\n",
    "    \"messages\": \"What did the president say about Ketanji Brown Jackson?\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1,\n",
    "    \"top_k\": -1,\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 650,\n",
    "    \"stop\": \"string\",\n",
    "    \"stream\": False,\n",
    "    \"user\": \"string\",\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eee5d3-fc28-40f6-b7b4-bd42a19beeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5404200-fdea-4ea1-bf27-7b6feb15d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hwchase17/langchain/v0.0.200/docs/modules/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1906b61-e7ec-46c0-91d6-198279265e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])\n",
    "llm = ChatOpenAI(model=\"eci-io/climategpt-7b\")\n",
    "\n",
    "questions = [\n",
    "    \"Who is the speaker\",\n",
    "    \"What did the president say about Ketanji Brown Jackson\",\n",
    "    \"What are the threats to America\",\n",
    "    \"Who are mentioned in the speech\",\n",
    "    \"Who is the vice president\",\n",
    "    \"How many projects were announced\",\n",
    "]\n",
    "\n",
    "for query in questions:\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Answer:\", index.query(query, llm=llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef680fc-40cf-4274-82f9-7f963b44c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answer:\", index.query(\"Where is San Diego?\", llm=llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c08b1-7718-43e5-8cf0-5e5ae2508048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answer:\", index.query(\"I want to go to San Diego from St. George Utah give me a plan\", llm=llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672120f7-3c1a-4a4a-8f61-13a4be4cb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answer:\", index.query(\"Who is narendra modi?\", llm=llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e930c-a9f3-4171-a4e3-00903f651eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
