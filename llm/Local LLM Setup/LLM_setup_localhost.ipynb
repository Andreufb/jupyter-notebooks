{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6df280-eca4-4c44-8361-8a6d347439bc",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 40px; margin-top: 0;\">\n",
    "    <div style=\"flex: 0 0 auto; margin-left: 0; margin-bottom: 0;margin-top: 0;\">\n",
    "        <img src=\"../pics/NationalDataPlatform_logo.png\" alt=\"WiFire Logo\" style=\"width: 179px; margin-bottom: 0px;\">\n",
    "    </div>    \n",
    "    <div style=\"flex: 0 0 auto; margin-left: auto; margin-bottom: 0; margin-top: 0;\">\n",
    "        <img src=\"../pics/logo_UCSD.png\" alt=\"UCSD Logo\" style=\"width: 179px; margin-bottom: 0px; margin-top: 20px;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 0 0 auto; margin-left: auto; margin-bottom: 0; margin-top: 20px;\">\n",
    "        <img src=\"../pics/sdsclogo-plusname-horiz-red.jpg\" alt=\"San Diego Supercomputer Center Logo\" width=\"300\"/>\n",
    "    </div>\n",
    "</div>\n",
    "<h1 style=\"text-align: center; font-size: 24px; margin-top: 0;\">NSF National Data Platform (NDP)</h1>\n",
    "<h3 style=\"text-align: center; font-size: 18px; margin-top: 10px;\">LLM as a Service Tutorial</h3>\n",
    "<div style=\"margin: 20px 0;\">\n",
    "    <p align=\"justify\"> Large Language Models are a powerful AI tool with multiple applications in both research and education, given their capacity to process big amounts of information to generate human-like language.</p>\n",
    "    <p align=\"justify\"> Understanding today's relevance of LLM's, the National Data Platform (NDP) has developed an LLM service to contribute to the research and education goals of its users.</p>\n",
    "    <p align=\"justify\"> In this guide, we are covering the use of an LLM as an NDP service. The main purpose of this demo is to showcase how this service works by submitting a series of sample queries, as well as comparing the performance when adding new documentation to a model. The main goal is to allow the user to identify the potential use cases of this service.</p>\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "    <div style=\"text-align: right; padding: 5px;\">\n",
    "        <p style=\"text-align: right;\"><strong>Contact:</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfzjlc0Sw2fTFTKArOZ0ffKNdVcPivf218kLXkBKfobGPbDMw/viewform\"> NDP Issue Reporting Form </a></p>\n",
    "    </div>\n",
    "</cente\n",
    "\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: flex-start; margin-top: 20px; border-top: 1px solid #ccc; padding-top: 20px;\">\n",
    "    <img src=\"https://new.nsf.gov/themes/custom/nsf_theme/components/images/logo/logo-desktop.svg\" alt=\"NSF Logo\" style=\"width: 120px; margin-right: 10px;\">\n",
    "    <p style=\"font-size: 12px;\">The National Data Platform was funded by NSF 2333609 under CI, CISE Research Resources programs. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funders.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39b811-7bb3-4b37-aeca-738674d650db",
   "metadata": {},
   "source": [
    "### What is an LLM?\n",
    "\n",
    "An LLM, or Large Language Model, is a type of artificial intelligence designed to understand and generate human-like text based on the data it's been trained on. By adding a vast amount of text from different sources and context's (web, books, papers, among others) LLM's are capable to identify the patterns of the human language under different contexts and provide responses to complex questions. \n",
    "\n",
    "LLM's posses a huge potential in both research and education, given their capability to quickly process and summarize a vast amount of information. LLM's can bee seen as a powerful tool to accelerate learning, facilitate the sharing of knowledge, process and generate big amounts of data, generate new hypothesis questions, among other uses.\n",
    "\n",
    "### ClimateGPT\n",
    "\n",
    "For this demo, we are using the [ClimateGPT](https://climategpt.ai/) model. Climate GPT was developed by team of researchers at RWTH Aachen University, in collaboration with Erasmus AI and others, for interdisciplinary research focused on climate change. They constructed 7B models from the ground up, utilizing a scientifically-oriented dataset of 300 billion tokens.  \n",
    "\n",
    "The model was made publicly available through [HuggingFace](https://huggingface.co/eci-io/climategpt-70b) in January 2024. It comes in well-maintained 7B, 13B, and 70B versions, built upon the Llama 2 architecture and leveraging a dataset of 4.2 billion tokens.\n",
    "\n",
    "### Hands on\n",
    "\n",
    "We will start using LLM as a service. First, we will make some questions to the vanilla ClimateGPT to see its overall performance. Then, we will extend the chorpus of the model by adding a new document. We expect the model to be able to answer questions based on the new document.\n",
    "\n",
    "### Local Set Up\n",
    "This notebook starts all required web servers on localhost (inside this Kubernetes pod)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1da733-7d5b-4c6a-815b-344dcc627a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "os.environ['HF_HOME']='/srv/starter_content/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfa3a4-ed7a-461a-a335-09cfeafc2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"eci-io/climategpt-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d7dbf-fc29-4cc4-a9a0-50ade36ba1b3",
   "metadata": {},
   "source": [
    "## NDP LLM Service Documentation\n",
    "\n",
    "This Python code snippet is designed to launch various components of a chat service named \"FastChat.\" Each function starts a different part of the service using the `subprocess.run` method to execute shell commands.\n",
    "\n",
    "### `run_controller()`\n",
    "\n",
    "Starts the controller for the FastChat service, responsible for managing and coordinating different parts of the service.\n",
    "\n",
    "```python\n",
    "def run_controller():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"127.0.0.1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153eb97-e79f-4592-8a49-5ce4ff1efe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_controller():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"127.0.0.1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730d1d8-8a13-4b7c-907c-03b948f6633b",
   "metadata": {},
   "source": [
    "## `run_worker`\n",
    "Initiates a model worker for processing and generating responses based on specified models. Runs the model worker module, specifying the local host and a list of model names for processing requests. The --model-path argument should point to the directory where the models are stored.\n",
    "```python \n",
    "def run_model_worker():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.model_worker\", \"--host\", \"127.0.0.1\", \"--model-names\", f\"{model},text-embedding-ada-002\", \"--model-path\", model])\n",
    "\n",
    "```\n",
    "### `run_api`\n",
    "\n",
    "Launches an API server that handles API requests to the FastChat service.\n",
    "Runs the API server module on the local host, acting as an interface between the service and external clients or applications.\n",
    "```python\n",
    "def run_api_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"127.0.0.1\"])\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879fbbe-fdfc-42a4-b4ed-cbab96c2a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_worker():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.model_worker\", \"--host\", \"127.0.0.1\", \"--model-names\", f\"{model},text-embedding-ada-002\", \"--model-path\", model])\n",
    "\n",
    "def run_api_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"127.0.0.1\"])\n",
    "def run_ui_server():\n",
    "    subprocess.run([\"python3\", \"-m\", \"fastchat.serve.gradio_web_server\", \"--host\", \"127.0.0.1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a7ae9-3ab4-460c-baf2-80a0ae43a45a",
   "metadata": {},
   "source": [
    "## Starting the `run_controller` Function in a Separate Thread\n",
    "\n",
    "To enable the FastChat controller to run concurrently with the main program, the `run_controller` function is executed in a separate thread. This is achieved using Python's `threading` module, which allows for the execution of code in parallel to the main execution flow of the program.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "controller_thread = threading.Thread(target=run_controller)\n",
    "controller_thread.start()\n",
    "```\n",
    "\n",
    "### Note: please wait for the following output line:\n",
    "```\n",
    "2024-03-14 20:35:37 | ERROR | stderr | INFO:     Uvicorn running on http://127.0.0.1:21001 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629becb-dfcd-4262-be6c-d3606e711ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_thread = threading.Thread(target=run_controller)\n",
    "controller_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789765a-af59-4fc7-80a7-06b32114b829",
   "metadata": {},
   "source": [
    "## Starting the `run_model_worker` Function in a Separate Thread\n",
    "\n",
    "To facilitate concurrent execution of the FastChat model worker alongside the main program and potentially other service components, the `run_model_worker` function is executed in a separate thread. This concurrent execution is made possible through the use of Python's `threading` module.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "model_worker_thread = threading.Thread(target=run_model_worker)\n",
    "model_worker_thread.start()\n",
    "```\n",
    "\n",
    "\n",
    "### Note: please wait for the following output line:\n",
    "```\n",
    "2024-03-14 20:36:18 | ERROR | stderr | INFO:     Uvicorn running on http://127.0.0.1:21002 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4eab9-b566-4230-8c5f-8fd73d6a8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_worker_thread = threading.Thread(target=run_model_worker)\n",
    "model_worker_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdd025-0d10-4b2a-94c5-a2fe681cf1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec420dc4-bfb8-4a29-b34b-1bab4a203385",
   "metadata": {},
   "source": [
    "## Running the `run_api_server` Function in a Separate Thread\n",
    "\n",
    "To ensure the API server component of the FastChat service operates concurrently with other parts of the application, the `run_api_server` function is launched in a separate thread. This concurrency is achieved with the help of Python's `threading` module, allowing multiple components to run simultaneously, improving scalability and responsiveness.\n",
    "\n",
    "### Code Snippet:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "api_server_thread = threading.Thread(target=run_api_server)\n",
    "api_server_thread.start()\n",
    "\n",
    "### Note: please wait for the following output line:\n",
    "```\n",
    "2024-03-14 20:35:37 | ERROR | stderr | INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d6983-f16e-417e-b648-fc918917c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_server_thread = threading.Thread(target=run_api_server)\n",
    "api_server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc776567-50d7-4d33-aba4-818269392384",
   "metadata": {},
   "source": [
    "## Test that everything works and ready (the response should contain the list of models and other parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889395f9-a7b1-413e-b67b-8f494abd5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://localhost:8000/v1/models').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f90cc-d1ff-4433-b75c-923fbd394630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
